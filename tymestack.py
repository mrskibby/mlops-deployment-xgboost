# -*- coding: utf-8 -*-
"""Tymestack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YIf-z3edmJ3kE7mQgGzUncD9wzzL4yBc
"""

import pandas as pd
import numpy as np

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Combine the data and target into a DataFrame for easier handling
columns = [
    "CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX",
    "PTRATIO", "B", "LSTAT", "PRICE"
]
df = pd.DataFrame(np.hstack([data, target.reshape(-1, 1)]), columns=columns)

# Show the first few rows
df.head()

from sklearn.model_selection import train_test_split

# Separate the features and target
X = df.drop("PRICE", axis=1)  # Features
y = df["PRICE"]  # Target

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the splits
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Initialize the RandomForestRegressor
rf_model = RandomForestRegressor(random_state=42)

# Train the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Random Forest MSE: {mse}")

import xgboost as xgb

# Initialize the XGBoost Regressor
xgb_model = xgb.XGBRegressor(random_state=42)

# Train the model on the training data
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"XGBoost MSE: {mse}")

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': np.arange(50, 500, 50),
    'max_depth': np.arange(3, 10, 1),
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
    'subsample': [0.6, 0.8, 1.0]
}

# Initialize the XGBoost Regressor
xgb_model = xgb.XGBRegressor(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    xgb_model, param_distributions=param_grid, n_iter=50,
    scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42, n_jobs=-1
)

# Fit the random search model
random_search.fit(X_train, y_train)

# Print the best parameters
print(f"Best Parameters: {random_search.best_params_}")

# Evaluate the best model on the test set
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Best XGBoost MSE: {mse}")

from sklearn.metrics import r2_score

# Calculate the R² score for XGBoost predictions
r2_xgb = r2_score(y_test, y_pred)
print(f"R² Score for XGBoost: {r2_xgb}")

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': np.arange(50, 500, 50),
    'max_depth': np.arange(3, 15, 1),
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)

# Initialize RandomizedSearchCV for Random Forest
random_search_rf = RandomizedSearchCV(
    rf_model, param_distributions=param_grid_rf, n_iter=50,
    scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42, n_jobs=-1
)

# Fit the random search model
random_search_rf.fit(X_train, y_train)

# Print the best parameters
print(f"Best Parameters for Random Forest: {random_search_rf.best_params_}")

# Evaluate the best model on the test set
best_rf_model = random_search_rf.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f"Best Random Forest MSE: {mse_rf}")

# Calculate the R² score
r2 = r2_score(y_test, y_pred)
print(f"R² Score: {r2}")

